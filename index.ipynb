{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, Dropout, Resizing, Input, Normalization\n",
    "from keras.models import Sequential\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We donwload and extract required dataset using `pathlib` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = 'data/'\n",
    "\n",
    "data_dir = pathlib.Path(DATASET_PATH)\n",
    "\n",
    "tf.keras.utils.get_file(\n",
    "    'voicedataset.zip',\n",
    "    origin='http://aiolearn.com/dl/datasets/voicedata.zip',\n",
    "    extract=True,\n",
    "    cache_dir='.',\n",
    "    cache_subdir='data'\n",
    ")\n",
    "\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['right',\n",
       " 'voicedataset.zip',\n",
       " 'go',\n",
       " 'no',\n",
       " 'left',\n",
       " 'stop',\n",
       " 'README.md',\n",
       " 'up',\n",
       " 'down',\n",
       " 'yes']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.io.gfile.listdir(str(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Commands:  ['right' 'go' 'no' 'left' 'stop' 'up' 'down' 'yes']\n"
     ]
    }
   ],
   "source": [
    "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "\n",
    "commands = commands[(commands != 'README.md') & (commands != 'voicedataset.zip')]\n",
    "\n",
    "print('Available Commands: ', commands )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 8 classes.\n",
      "Using 6400 files for training.\n",
      "Using 1600 files for validation.\n",
      "\n",
      "label :  ['down' 'go' 'left' 'no' 'right' 'stop' 'up' 'yes']\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = tf.keras.utils.audio_dataset_from_directory(directory=data_dir,\n",
    "                                                              batch_size=64,\n",
    "                                                              validation_split=0.2,\n",
    "                                                              seed=0,\n",
    "                                                              output_sequence_length=16000,\n",
    "                                                              subset='both')\n",
    "\n",
    "label_names = np.array(X_train.class_names)\n",
    "print()\n",
    "print('label : ', label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we mix and merge the sound and its label with the `squeeze` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze(audio, labels):\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    return audio, labels\n",
    "\n",
    "X_train = X_train.map(squeeze, tf.data.AUTOTUNE)\n",
    "X_test = X_test.map(squeeze, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 16000), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None,), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val = X_test\n",
    "X_test = X_test.shard(num_shards=2,\n",
    "                   index=0)\n",
    "val = X_test.shard(num_shards=2,\n",
    "                   index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, with the ‍‍‍‍`take(1)` command, the X and Y values ​​of the first patch created are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.4648438e-03  4.5776367e-03  2.9602051e-03 ...  3.2653809e-03\n",
      "   2.5634766e-03  3.6926270e-03]\n",
      " [ 6.9885254e-03  5.8898926e-03  6.7749023e-03 ...  9.4604492e-03\n",
      "   8.2092285e-03  8.0566406e-03]\n",
      " [ 0.0000000e+00  8.5449219e-04 -3.9672852e-04 ...  3.7231445e-03\n",
      "  -1.2817383e-03 -3.2043457e-03]\n",
      " ...\n",
      " [-1.3122559e-03 -1.6174316e-03 -2.5329590e-03 ... -2.9602051e-03\n",
      "  -2.7770996e-03 -2.4719238e-03]\n",
      " [ 6.1035156e-05  3.0517578e-05 -3.0517578e-05 ... -1.5258789e-04\n",
      "   3.0517578e-05  2.1362305e-04]\n",
      " [-3.0517578e-04 -2.2277832e-03 -2.8686523e-03 ... -3.0517578e-05\n",
      "   2.7465820e-04  1.0375977e-03]], shape=(64, 16000), dtype=float32)\n",
      "['down' 'go' 'right' 'no' 'down' 'left' 'yes' 'go' 'stop' 'go' 'left' 'up'\n",
      " 'down' 'left' 'left' 'down' 'down' 'down' 'go' 'no' 'yes' 'right' 'up'\n",
      " 'down' 'down' 'no' 'up' 'up' 'go' 'yes' 'yes' 'no' 'stop' 'stop' 'up'\n",
      " 'go' 'right' 'stop' 'left' 'down' 'no' 'down' 'down' 'up' 'no' 'down'\n",
      " 'down' 'go' 'stop' 'yes' 'no' 'stop' 'up' 'stop' 'right' 'go' 'yes' 'up'\n",
      " 'down' 'left' 'go' 'up' 'down' 'no']\n"
     ]
    }
   ],
   "source": [
    "for x, y in X_train.take(1):\n",
    "    print(x)\n",
    "    print(label_names[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16000)\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "for example_audio, example_labels in X_train.take(1):\n",
    "    print(example_audio.shape)\n",
    "    print(example_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
